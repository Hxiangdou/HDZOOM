from random import uniform

import os 
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"


import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
from torchvision.transforms import functional as TF
import torch.nn.functional as F
from IPython.core.debugger import set_trace 
from dataloader.creatidesign_dataset_benchmark import DesignDataset,visualize_bbox,collate_fn,tensor_to_pil,make_image_grid_RGB
import numpy as np
from PIL import Image
from safetensors.torch import save_file, load_file
from accelerate import load_checkpoint_and_dispatch
from modules.flux.transformer_flux_creatidesign import FluxTransformer2DModel
from pipeline.pipeline_flux_creatidesign import FluxPipeline
import json
from huggingface_hub import snapshot_download
from modules.flux.attention_processor_flux_creatidesign import (
    FluxInvertedSwinPostProcessor, 
    Attention,
    DesignFluxAttnProcessor2_0 # æˆ–è€…æ˜¯æ¨¡å‹å½“å‰ä½¿ç”¨çš„ Processor ç±»
)
from diffusers.optimization import get_scheduler
import math
from accelerate import Accelerator
from accelerate.utils import ProjectConfiguration
import logging
from diffusers import FluxPipeline
from diffusers.optimization import get_scheduler
from accelerate.logging import get_logger

logger = get_logger(__name__)

from modules.flux.attention_processor_flux_creatidesign import (
    FluxInvertedSwinPostProcessor,
    Attention
)
from config import Config
import random
config = Config()


def save_adapter_weights(accelerator, model, output_dir, step):
    """
    ä»…ä¿å­˜ InvertedSwinModule çš„æƒé‡ï¼Œä¸ä¿å­˜æ•´ä¸ª Flux æ¨¡å‹ã€‚
    """
    if accelerator.is_main_process:
        save_path = os.path.join(output_dir, f"checkpoint-{step}")
        os.makedirs(save_path, exist_ok=True)
        
        # è·å–æœªå°è£…çš„æ¨¡å‹ (å»é™¤ DDP/Accelerate åŒ…è£…)
        unwrapped_model = accelerator.unwrap_model(model)
        state_dict = {}
        count = 0
        
        for name, module in unwrapped_model.named_modules():
            if isinstance(module, FluxInvertedSwinPostProcessor):
                prefix = f"{name}.swin_module"
                # ä»…æå– swin_module çš„å‚æ•°
                for param_name, param in module.swin_module.state_dict().items():
                    state_dict[f"{prefix}.{param_name}"] = param.cpu()
                count += 1
        
        if count > 0:
            torch.save(state_dict, os.path.join(save_path, "swin_adapter.pt"))
            logger.info(f"ğŸ’¾ [Step {step}] Saved {count} adapter modules to {save_path}")
        else:
            logger.warning("âš ï¸ No adapter modules found to save!")


def main():

    print(f"ğŸš€æ­£åœ¨åŠ è½½æ¨¡å‹: HDZOOM")
    model_path = "./black-forest-labs/FLUX.1-dev"

    ckpt_repo = "HuiZhang0812/CreatiDesign" # huggingface repo of ckpt

    ckpt_path = snapshot_download(
        repo_id=ckpt_repo,
        repo_type="model",
        local_dir="./CreatiDesign_checkpoint",
        local_dir_use_symlinks=False
    )

    # Load transformer config from checkpoint
    with open(os.path.join(ckpt_path, "transformer", "config.json"), 'r') as f:
        transformer_config = json.load(f)
    
    transformer = FluxTransformer2DModel(**transformer_config)
    transformer = load_checkpoint_and_dispatch(transformer, checkpoint=os.path.join(model_path,"transformer"), device_map=None)

    # Load lora parameters using safetensors
    state_dict = load_file(os.path.join(ckpt_path, "transformer","model.safetensors"))

    # Load parameters, allow partial loading
    missing_keys, unexpected_keys = transformer.load_state_dict(state_dict, strict=False)
    
    print(f"Loaded parameters: {len(state_dict)}",state_dict.keys())
    print(f"Missing keys: {len(missing_keys)}",missing_keys)
    print(f"Unexpected keys: {len(unexpected_keys)}",unexpected_keys)

    transformer = transformer.to(dtype=torch.bfloat16)

    pipe = FluxPipeline.from_pretrained(model_path, transformer=transformer,torch_dtype=torch.bfloat16)
    
    # Latent åˆ†è¾¨ç‡ (Flux ç»è¿‡ VAE 8x å’Œ Patch 2xï¼Œæ€»å…±ç¼©å° 16 å€)
    latent_resolution = (config.resolution // 16, config.resolution // 16)  # ç»“æœä¸º (64, 64)
    
    for name, module in pipe.transformer.named_modules():
        # ä»…é’ˆå¯¹ SingleTransformerBlock ä¸­çš„ Attention (è‡ªæ³¨æ„åŠ›)
        if "single_transformer_blocks" in name and isinstance(module, Attention):
        
            current_processor = module.processor
            
            dim = module.out_dim if module.out_dim is not None else module.query_dim
            # å¯¹äº Flux.1-devï¼Œè¿™é‡Œé€šå¸¸æ˜¯ 3072
            dim = min(dim, 192)  # ç¡®ä¿ä¸è¶…è¿‡ 4096
            window_size = 8
            
            # æ£€æŸ¥åˆ†è¾¨ç‡æ˜¯å¦åŒ¹é…çª—å£
            if latent_resolution[0] % window_size != 0 or latent_resolution[1] % window_size != 0:
                print(f"è­¦å‘Š: åˆ†è¾¨ç‡ {latent_resolution} ä¸èƒ½è¢« window_size {window_size} æ•´é™¤ï¼Œå¯èƒ½ä¼šæŠ¥é”™ã€‚")
            
            swin_num_heads = module.heads  
            if dim % swin_num_heads != 0:
                raise ValueError(f"ç»´åº¦ {dim} æ— æ³•è¢«å¤´æ•° {swin_num_heads} æ•´é™¤")

            depths = [2, 2, 2, 2]  # æ ¹æ®æ˜¾å­˜æƒ…å†µè°ƒæ•´

            # print(f"æ­£åœ¨æ³¨å…¥å±‚: {name}")
            # print(f"  - Dim: {dim}")
            # print(f"  - Resolution: {latent_resolution}")
            
            # --- å®ä¾‹åŒ–å¹¶æ›¿æ¢ ---
            
            # å®ä¾‹åŒ–ä½ çš„ Wrapper Processor
            swin_wrapper = FluxInvertedSwinPostProcessor(
                base_processor=current_processor,
                in_dim=dim,
                input_resolution=latent_resolution,
                depths=depths,
                num_heads=swin_num_heads,
                window_size=window_size
            ).to(pipe.device, dtype=pipe.dtype)
            # print(f"  - ä½¿ç”¨çš„ Processor: {swin_wrapper.__class__.__name__}")
            # æ›¿æ¢
            module.set_processor(swin_wrapper)
    pipe = pipe.to("cuda")

    print("â„ï¸ æ­£åœ¨å†»ç»“åŸºç¡€æ¨¡å‹ï¼Œä»…è§£å†» HDZOOM æ¨¡å—...")
    
    # å…¨å±€å†»ç»“
    pipe.transformer.requires_grad_(False)
    pipe.vae.requires_grad_(False)
    pipe.text_encoder.requires_grad_(False)
    pipe.text_encoder_2.requires_grad_(False)
    
    # å±€éƒ¨è§£å†»
    trainable_params = []
    for name, module in pipe.transformer.named_modules():
        if isinstance(module, FluxInvertedSwinPostProcessor):
            # ä»…è§£å†» swin_module
            module.swin_module.requires_grad_(True)
            trainable_params.extend(module.swin_module.parameters())

    print(f"ğŸ”¥ å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params)}")
    
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.learning_rate,
        weight_decay=1e-2
    )

    print("ğŸ“¦ å‡†å¤‡æ•°æ®...")
    # benchmark_repo = 'HuiZhang0812/CreatiDesign_benchmark' #  huggingface repo of benchmark
    benchmark_repo = '/home/fength/.cache/huggingface/datasets/HuiZhang0812___creati_design_benchmark/default/0.0.0/63fb381622f01b2f3ee11e56f0a1a017d52a843d/'
    # benchmark_repo = 'HuiZhang0812/CreatiDesign_benchmark' #  huggingface repo of benchmark
    train_datasets = DesignDataset(dataset_name=benchmark_repo,
                             resolution=config.resolution,
                             condition_resolution=config.condition_resolution,
                             neg_condition_image =config.neg_condition_image,
                             background_color=config.background_color,
                             use_bucket=config.use_bucket,
                             condition_resolution_scale_ratio=config.condition_resolution_scale_ratio,
                             split="test",
                             )
    train_dataloader = DataLoader(train_datasets, batch_size=config.batch_size, shuffle=True, num_workers=4,collate_fn=collate_fn)
    test_datasets = DesignDataset(dataset_name=benchmark_repo,
                                resolution=config.resolution,
                                condition_resolution=config.condition_resolution,
                                neg_condition_image =config.neg_condition_image,
                                background_color=config.background_color,
                                use_bucket=config.use_bucket,
                                condition_resolution_scale_ratio=config.condition_resolution_scale_ratio,
                                split="test",
                                )
    test_dataloader = DataLoader(test_datasets, batch_size=1, shuffle=False, num_workers=4,collate_fn=collate_fn)
    
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / config.gradient_accumulation_steps)
    max_train_steps = config.num_epochs * num_update_steps_per_epoch

    print(f"ğŸ§® é¢„è®¡æ€»æ›´æ–°æ­¥æ•°: {max_train_steps} (é¢„çƒ­: {config.lr_warmup_steps})")
    
    
    
    # åˆå§‹åŒ– Accelerator
    accelerator_project_config = ProjectConfiguration(project_dir=config.output_dir, logging_dir=os.path.join(config.output_dir, "logs"))
    accelerator = Accelerator(
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        mixed_precision=config.mixed_precision, # è¯»å– config.py ä¸­çš„ "bf16"
        log_with="tensorboard",
        project_config=accelerator_project_config
    )
    
    lr_scheduler = get_scheduler(
        config.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=config.lr_warmup_steps * accelerator.num_processes, # å¦‚æœæ˜¯å¤šå¡ï¼Œé¢„çƒ­æ­¥æ•°å¯èƒ½éœ€è¦è°ƒæ•´ï¼Œé€šå¸¸ä¿æŒåŸå€¼å³å¯
        num_training_steps=max_train_steps,
    )
    
    pipe.vae.to(accelerator.device)
    pipe.text_encoder.to(accelerator.device)
    pipe.text_encoder_2.to(accelerator.device)
    # åˆå§‹åŒ–æ—¥å¿—
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger = logging.getLogger(__name__)
    logger.info(accelerator.state)

    # å‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
    
    pipe.transformer, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        pipe.transformer, optimizer, train_dataloader, lr_scheduler
    )


    # å¼€å§‹è®­ç»ƒå¾ªç¯
    if accelerator.is_main_process:
        print("ğŸš€ Accelerate è®­ç»ƒç¯å¢ƒå·²å¯åŠ¨ï¼")
        print(f"   Batch size: {config.batch_size}")
        print(f"   Grad Accumulation: {config.gradient_accumulation_steps}")
        print(f"   Mixed Precision: {config.mixed_precision}")

    global_step = 0
    pipe.transformer.train()

    for epoch in range(config.num_epochs):
        # è¿›åº¦æ¡
        progress_bar = tqdm(
            total=len(train_dataloader), 
            disable=not accelerator.is_local_main_process, 
            desc=f"Epoch {epoch}"
        )
        
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(pipe.transformer):
                
                with torch.no_grad():
                    imgs = batch['img'].to(accelerator.device, dtype=pipe.dtype) 
                
                    prompts = batch["caption"] 
                    imgs_id = batch['id']
                    objects_boxes = batch["objects_boxes"]
                    objects_caption = batch['objects_caption'] 
                    objects_masks = batch['objects_masks']
                    condition_img = batch['condition_img']
                    neg_condition_img = batch['neg_condtion_img']
                    objects_masks_maps= batch['objects_masks_maps']
                    subject_masks_maps = batch['condition_img_masks_maps']
                    target_width=batch['target_width'][0]
                    target_height=batch['target_height'][0]

                    img_info = batch["img_info"][0] 
                    filename = img_info["img_id"]+'.jpg'
                    start_time = time.time()

                    # <--- æ–°å¢: CFG Dropout é€»è¾‘ (éšæœºä¸¢å¼ƒæ¡ä»¶)
                    # å‡è®¾ 10% çš„æ¦‚ç‡ä¸¢å¼ƒæ¡ä»¶ (è®­ç»ƒ unconditional åˆ†æ”¯)
                    if config.cfg_dropout_prob > 0:
                        # ç”Ÿæˆä¸€ä¸ª mask, True è¡¨ç¤ºä¸¢å¼ƒæ¡ä»¶
                        dropout_mask = torch.rand(B, device=accelerator.device) < config.cfg_dropout_prob
                        
                        # 1. æ›¿æ¢ Caption ä¸ºç©ºå­—ç¬¦ä¸²
                        captions = ["" if drop else cap for drop, cap in zip(dropout_mask, captions)]
                        
                        # 2. æ›¿æ¢ Condition Image ä¸º Negative Image (é€šå¸¸æ˜¯å…¨é»‘/å…¨ç™½/å…¨ç°)
                        # dropout_mask reshape for broadcast: [B] -> [B, 1, 1, 1]
                        mask_broadcast = dropout_mask[:, None, None, None].to(dtype=pipe.dtype)
                        final_condition_imgs = (1 - mask_broadcast) * condition_img + mask_broadcast * neg_condition_img
                    else:
                        final_condition_imgs = condition_img
                    # VAE ç¼–ç  (Pixel -> Latent)
                    latents = pipe.vae.encode(imgs).latent_dist.sample()
                    latents = (latents - pipe.vae.config.shift_factor) * pipe.vae.config.scaling_factor
                    
                    condition_latents = pipe.vae.encode(final_condition_imgs).latent_dist.sample()
                    condition_latents = (condition_latents - pipe.vae.config.shift_factor) * pipe.vae.config.scaling_factor
                    # Latent å˜å½¢ (Packing)
                    # Flux éœ€è¦ (B, L, C) æ ¼å¼
                    B, C, H, W = latents.shape
                    # ç®€å•å±•å¹³ (æ³¨æ„ï¼šFlux å®˜æ–¹æœ‰æ›´å¤æ‚çš„ patch æ‰“åŒ…ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–ç‰ˆé€šè¿‡ Reshape)
                    # å¦‚æœè¿™æ­¥æŠ¥é”™ï¼Œè¯´æ˜éœ€è¦å¼•å…¥å®˜æ–¹çš„ _pack_latents å‡½æ•°
                    latents = latents.view(B, C, -1).permute(0, 2, 1) # (B, H*W, C)

                    # æ–‡æœ¬ç¼–ç  (T5 + CLIP)
                    # ä½¿ç”¨ Pipeline çš„ encode_prompt æ–¹æ³•
                    prompt_embeds, pooled_prompt_embeds, text_ids = pipe.encode_prompt(
                        tokenizer, text_encoder, tokenizer_2, text_encoder_2, 
                        captions, max_sequence_length=512, device=accelerator.device
                    )
                    
                    # ç”Ÿæˆå™ªå£°å’Œæ—¶é—´æ­¥
                    noise = torch.randn_like(latents)
                    # éšæœºæ—¶é—´æ­¥
                    timesteps = torch.rand((B,), device=accelerator.device)
                    
                    # åŠ å™ª (Flow Matching: x_t = (1-t)x_0 + t*noise)
                    noisy_latents = (1 - timesteps.view(B, 1, 1)) * latents + timesteps.view(B, 1, 1) * noise

                    # å‡†å¤‡ img_ids
                    img_ids = pipe._prepare_latent_image_ids(B, config.resolution, config.resolution, accelerator.device, config.torch_dtype)

                # å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­
                noisy_latents.requires_grad_(True)
                
                # Predict Noise / Velocity
                model_pred = pipe.transformer(
                    hidden_states=noisy_latents,
                    timestep=timesteps, # Flux transformer æ¥å— float timesteps
                    guidance=torch.tensor([1.0]*B, device=accelerator.device), # è®­ç»ƒæ—¶é€šå¸¸ guidance=1
                    pooled_projections=pooled_prompt_embeds,
                    encoder_hidden_states=prompt_embeds,
                    txt_ids=text_ids,
                    img_ids=img_ids,
                    return_dict=False
                )[0]

                target = noise - latents
                
                loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")

                accelerator.backward(loss)
                
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(pipe.transformer.parameters(), 1.0)
                
                optimizer.step()
                lr_scheduler.step() 
                optimizer.zero_grad()

            # æ—¥å¿—ä¸ä¿å­˜
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                
                # æ‰“å°æ—¥å¿—
                if global_step % 10 == 0:
                    logger.info(f"Step {global_step}, Loss: {loss.item():.4f}")
                    accelerator.log({"loss": loss.item()}, step=global_step)

                # ä¿å­˜æƒé‡
                if global_step % config.save_steps == 0:
                    if accelerator.is_main_process:
                        save_adapter_weights(accelerator, pipe.transformer, config.output_dir, global_step)

    # è®­ç»ƒç»“æŸä¿å­˜
    save_adapter_weights(accelerator, pipe.transformer, config.output_dir, "final")
    accelerator.end_training()
    
    print("ğŸ‰ è®­ç»ƒç»“æŸï¼")

if __name__ == "__main__":
    main()